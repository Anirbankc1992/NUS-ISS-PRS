{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "numpy.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pima indians dataset\n",
    "dataset = numpy.loadtxt(\"pima-indians-diabetes.csv\", delimiter=\",\")\n",
    "# split into input (X) and output (Y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=1/5,random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train = scaler.transform(X_train)  \n",
    "X_test = scaler.transform(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.97160896\n",
      "Iteration 2, loss = 0.95444819\n",
      "Iteration 3, loss = 0.93863044\n",
      "Iteration 4, loss = 0.92400628\n",
      "Iteration 5, loss = 0.90967147\n",
      "Iteration 6, loss = 0.89774352\n",
      "Iteration 7, loss = 0.88593485\n",
      "Iteration 8, loss = 0.87479704\n",
      "Iteration 9, loss = 0.86515497\n",
      "Iteration 10, loss = 0.85457691\n",
      "Iteration 11, loss = 0.84546045\n",
      "Iteration 12, loss = 0.83640895\n",
      "Iteration 13, loss = 0.82762082\n",
      "Iteration 14, loss = 0.81932696\n",
      "Iteration 15, loss = 0.81070238\n",
      "Iteration 16, loss = 0.80212755\n",
      "Iteration 17, loss = 0.79332665\n",
      "Iteration 18, loss = 0.78506008\n",
      "Iteration 19, loss = 0.77662082\n",
      "Iteration 20, loss = 0.76838257\n",
      "Iteration 21, loss = 0.76050661\n",
      "Iteration 22, loss = 0.75224660\n",
      "Iteration 23, loss = 0.74335706\n",
      "Iteration 24, loss = 0.73410132\n",
      "Iteration 25, loss = 0.72471007\n",
      "Iteration 26, loss = 0.71483759\n",
      "Iteration 27, loss = 0.70539288\n",
      "Iteration 28, loss = 0.69528538\n",
      "Iteration 29, loss = 0.68490927\n",
      "Iteration 30, loss = 0.67382252\n",
      "Iteration 31, loss = 0.66312443\n",
      "Iteration 32, loss = 0.65211214\n",
      "Iteration 33, loss = 0.64185468\n",
      "Iteration 34, loss = 0.63198455\n",
      "Iteration 35, loss = 0.62276746\n",
      "Iteration 36, loss = 0.61502897\n",
      "Iteration 37, loss = 0.60730208\n",
      "Iteration 38, loss = 0.60067562\n",
      "Iteration 39, loss = 0.59391956\n",
      "Iteration 40, loss = 0.58736462\n",
      "Iteration 41, loss = 0.58078730\n",
      "Iteration 42, loss = 0.57416235\n",
      "Iteration 43, loss = 0.56721427\n",
      "Iteration 44, loss = 0.56114580\n",
      "Iteration 45, loss = 0.55524709\n",
      "Iteration 46, loss = 0.54968658\n",
      "Iteration 47, loss = 0.54424611\n",
      "Iteration 48, loss = 0.53930957\n",
      "Iteration 49, loss = 0.53468840\n",
      "Iteration 50, loss = 0.53011826\n",
      "Iteration 51, loss = 0.52576841\n",
      "Iteration 52, loss = 0.52163565\n",
      "Iteration 53, loss = 0.51755796\n",
      "Iteration 54, loss = 0.51392981\n",
      "Iteration 55, loss = 0.51030264\n",
      "Iteration 56, loss = 0.50700705\n",
      "Iteration 57, loss = 0.50391571\n",
      "Iteration 58, loss = 0.50068673\n",
      "Iteration 59, loss = 0.49814163\n",
      "Iteration 60, loss = 0.49560033\n",
      "Iteration 61, loss = 0.49334471\n",
      "Iteration 62, loss = 0.49119702\n",
      "Iteration 63, loss = 0.48905765\n",
      "Iteration 64, loss = 0.48699787\n",
      "Iteration 65, loss = 0.48521647\n",
      "Iteration 66, loss = 0.48338151\n",
      "Iteration 67, loss = 0.48211186\n",
      "Iteration 68, loss = 0.48062153\n",
      "Iteration 69, loss = 0.47948653\n",
      "Iteration 70, loss = 0.47837390\n",
      "Iteration 71, loss = 0.47750466\n",
      "Iteration 72, loss = 0.47643339\n",
      "Iteration 73, loss = 0.47550770\n",
      "Iteration 74, loss = 0.47471002\n",
      "Iteration 75, loss = 0.47386807\n",
      "Iteration 76, loss = 0.47314784\n",
      "Iteration 77, loss = 0.47224876\n",
      "Iteration 78, loss = 0.47138146\n",
      "Iteration 79, loss = 0.47065013\n",
      "Iteration 80, loss = 0.46982528\n",
      "Iteration 81, loss = 0.46901803\n",
      "Iteration 82, loss = 0.46845240\n",
      "Iteration 83, loss = 0.46783388\n",
      "Iteration 84, loss = 0.46727295\n",
      "Iteration 85, loss = 0.46672735\n",
      "Iteration 86, loss = 0.46601119\n",
      "Iteration 87, loss = 0.46543141\n",
      "Iteration 88, loss = 0.46483839\n",
      "Iteration 89, loss = 0.46436349\n",
      "Iteration 90, loss = 0.46378527\n",
      "Iteration 91, loss = 0.46323737\n",
      "Iteration 92, loss = 0.46270792\n",
      "Iteration 93, loss = 0.46223629\n",
      "Iteration 94, loss = 0.46183377\n",
      "Iteration 95, loss = 0.46119577\n",
      "Iteration 96, loss = 0.46074600\n",
      "Iteration 97, loss = 0.46021687\n",
      "Iteration 98, loss = 0.45957292\n",
      "Iteration 99, loss = 0.45919153\n",
      "Iteration 100, loss = 0.45876338\n",
      "Iteration 101, loss = 0.45827366\n",
      "Iteration 102, loss = 0.45796959\n",
      "Iteration 103, loss = 0.45764911\n",
      "Iteration 104, loss = 0.45733492\n",
      "Iteration 105, loss = 0.45683318\n",
      "Iteration 106, loss = 0.45633862\n",
      "Iteration 107, loss = 0.45593952\n",
      "Iteration 108, loss = 0.45526552\n",
      "Iteration 109, loss = 0.45476951\n",
      "Iteration 110, loss = 0.45434042\n",
      "Iteration 111, loss = 0.45375014\n",
      "Iteration 112, loss = 0.45314422\n",
      "Iteration 113, loss = 0.45277286\n",
      "Iteration 114, loss = 0.45231131\n",
      "Iteration 115, loss = 0.45199757\n",
      "Iteration 116, loss = 0.45140597\n",
      "Iteration 117, loss = 0.45102162\n",
      "Iteration 118, loss = 0.45069482\n",
      "Iteration 119, loss = 0.45025135\n",
      "Iteration 120, loss = 0.45008220\n",
      "Iteration 121, loss = 0.44977826\n",
      "Iteration 122, loss = 0.44946617\n",
      "Iteration 123, loss = 0.44918715\n",
      "Iteration 124, loss = 0.44880088\n",
      "Iteration 125, loss = 0.44850661\n",
      "Iteration 126, loss = 0.44839903\n",
      "Iteration 127, loss = 0.44811925\n",
      "Iteration 128, loss = 0.44775925\n",
      "Iteration 129, loss = 0.44728172\n",
      "Iteration 130, loss = 0.44685470\n",
      "Iteration 131, loss = 0.44641611\n",
      "Iteration 132, loss = 0.44599119\n",
      "Iteration 133, loss = 0.44560143\n",
      "Iteration 134, loss = 0.44524165\n",
      "Iteration 135, loss = 0.44479872\n",
      "Iteration 136, loss = 0.44444017\n",
      "Iteration 137, loss = 0.44403417\n",
      "Iteration 138, loss = 0.44370858\n",
      "Iteration 139, loss = 0.44349032\n",
      "Iteration 140, loss = 0.44329468\n",
      "Iteration 141, loss = 0.44313627\n",
      "Iteration 142, loss = 0.44285956\n",
      "Iteration 143, loss = 0.44260356\n",
      "Iteration 144, loss = 0.44244414\n",
      "Iteration 145, loss = 0.44208214\n",
      "Iteration 146, loss = 0.44182245\n",
      "Iteration 147, loss = 0.44155468\n",
      "Iteration 148, loss = 0.44128535\n",
      "Iteration 149, loss = 0.44099164\n",
      "Iteration 150, loss = 0.44075926\n",
      "Iteration 151, loss = 0.44047442\n",
      "Iteration 152, loss = 0.44017829\n",
      "Iteration 153, loss = 0.43991837\n",
      "Iteration 154, loss = 0.43961975\n",
      "Iteration 155, loss = 0.43920780\n",
      "Iteration 156, loss = 0.43924372\n",
      "Iteration 157, loss = 0.43925375\n",
      "Iteration 158, loss = 0.43916599\n",
      "Iteration 159, loss = 0.43888150\n",
      "Iteration 160, loss = 0.43863476\n",
      "Iteration 161, loss = 0.43831537\n",
      "Iteration 162, loss = 0.43810056\n",
      "Iteration 163, loss = 0.43776956\n",
      "Iteration 164, loss = 0.43757806\n",
      "Iteration 165, loss = 0.43744531\n",
      "Iteration 166, loss = 0.43727152\n",
      "Iteration 167, loss = 0.43715534\n",
      "Iteration 168, loss = 0.43693171\n",
      "Iteration 169, loss = 0.43674852\n",
      "Iteration 170, loss = 0.43689132\n",
      "Iteration 171, loss = 0.43673444\n",
      "Iteration 172, loss = 0.43649230\n",
      "Iteration 173, loss = 0.43618730\n",
      "Iteration 174, loss = 0.43597008\n",
      "Iteration 175, loss = 0.43587376\n",
      "Iteration 176, loss = 0.43571418\n",
      "Iteration 177, loss = 0.43544579\n",
      "Iteration 178, loss = 0.43513131\n",
      "Iteration 179, loss = 0.43491585\n",
      "Iteration 180, loss = 0.43472432\n",
      "Iteration 181, loss = 0.43459653\n",
      "Iteration 182, loss = 0.43451971\n",
      "Iteration 183, loss = 0.43432224\n",
      "Iteration 184, loss = 0.43419270\n",
      "Iteration 185, loss = 0.43405994\n",
      "Iteration 186, loss = 0.43390345\n",
      "Iteration 187, loss = 0.43366650\n",
      "Iteration 188, loss = 0.43360503\n",
      "Iteration 189, loss = 0.43337947\n",
      "Iteration 190, loss = 0.43327143\n",
      "Iteration 191, loss = 0.43322796\n",
      "Iteration 192, loss = 0.43311548\n",
      "Iteration 193, loss = 0.43301606\n",
      "Iteration 194, loss = 0.43294967\n",
      "Iteration 195, loss = 0.43301487\n",
      "Iteration 196, loss = 0.43290503\n",
      "Iteration 197, loss = 0.43266880\n",
      "Iteration 198, loss = 0.43251954\n",
      "Iteration 199, loss = 0.43218540\n",
      "Iteration 200, loss = 0.43207233\n",
      "Iteration 201, loss = 0.43207487\n",
      "Iteration 202, loss = 0.43198992\n",
      "Iteration 203, loss = 0.43182240\n",
      "Iteration 204, loss = 0.43153379\n",
      "Iteration 205, loss = 0.43117407\n",
      "Iteration 206, loss = 0.43095042\n",
      "Iteration 207, loss = 0.43059471\n",
      "Iteration 208, loss = 0.43046528\n",
      "Iteration 209, loss = 0.43033973\n",
      "Iteration 210, loss = 0.43014563\n",
      "Iteration 211, loss = 0.42998602\n",
      "Iteration 212, loss = 0.42972661\n",
      "Iteration 213, loss = 0.42988207\n",
      "Iteration 214, loss = 0.42958901\n",
      "Iteration 215, loss = 0.42931032\n",
      "Iteration 216, loss = 0.42894132\n",
      "Iteration 217, loss = 0.42873270\n",
      "Iteration 218, loss = 0.42857088\n",
      "Iteration 219, loss = 0.42839106\n",
      "Iteration 220, loss = 0.42811951\n",
      "Iteration 221, loss = 0.42800196\n",
      "Iteration 222, loss = 0.42770654\n",
      "Iteration 223, loss = 0.42770470\n",
      "Iteration 224, loss = 0.42766531\n",
      "Iteration 225, loss = 0.42745334\n",
      "Iteration 226, loss = 0.42706198\n",
      "Iteration 227, loss = 0.42674502\n",
      "Iteration 228, loss = 0.42668671\n",
      "Iteration 229, loss = 0.42628617\n",
      "Iteration 230, loss = 0.42626020\n",
      "Iteration 231, loss = 0.42602266\n",
      "Iteration 232, loss = 0.42587855\n",
      "Iteration 233, loss = 0.42599836\n",
      "Iteration 234, loss = 0.42577643\n",
      "Iteration 235, loss = 0.42574845\n",
      "Iteration 236, loss = 0.42580833\n",
      "Iteration 237, loss = 0.42585922\n",
      "Iteration 238, loss = 0.42565627\n",
      "Iteration 239, loss = 0.42550598\n",
      "Iteration 240, loss = 0.42538166\n",
      "Iteration 241, loss = 0.42521090\n",
      "Iteration 242, loss = 0.42508694\n",
      "Iteration 243, loss = 0.42505725\n",
      "Iteration 244, loss = 0.42495762\n",
      "Iteration 245, loss = 0.42466612\n",
      "Iteration 246, loss = 0.42459882\n",
      "Iteration 247, loss = 0.42453938\n",
      "Iteration 248, loss = 0.42450136\n",
      "Iteration 249, loss = 0.42424339\n",
      "Iteration 250, loss = 0.42392140\n",
      "Iteration 251, loss = 0.42375527\n",
      "Iteration 252, loss = 0.42353217\n",
      "Iteration 253, loss = 0.42337745\n",
      "Iteration 254, loss = 0.42329519\n",
      "Iteration 255, loss = 0.42322860\n",
      "Iteration 256, loss = 0.42314108\n",
      "Iteration 257, loss = 0.42297573\n",
      "Iteration 258, loss = 0.42264481\n",
      "Iteration 259, loss = 0.42255677\n",
      "Iteration 260, loss = 0.42251656\n",
      "Iteration 261, loss = 0.42233831\n",
      "Iteration 262, loss = 0.42212645\n",
      "Iteration 263, loss = 0.42186006\n",
      "Iteration 264, loss = 0.42183968\n",
      "Iteration 265, loss = 0.42174885\n",
      "Iteration 266, loss = 0.42159432\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 267, loss = 0.42145041\n",
      "Iteration 268, loss = 0.42136293\n",
      "Iteration 269, loss = 0.42133664\n",
      "Iteration 270, loss = 0.42110277\n",
      "Iteration 271, loss = 0.42109857\n",
      "Iteration 272, loss = 0.42102132\n",
      "Iteration 273, loss = 0.42076405\n",
      "Iteration 274, loss = 0.42068491\n",
      "Iteration 275, loss = 0.42059837\n",
      "Iteration 276, loss = 0.42037380\n",
      "Iteration 277, loss = 0.42004823\n",
      "Iteration 278, loss = 0.41994636\n",
      "Iteration 279, loss = 0.41984266\n",
      "Iteration 280, loss = 0.41978477\n",
      "Iteration 281, loss = 0.41964753\n",
      "Iteration 282, loss = 0.41966998\n",
      "Iteration 283, loss = 0.41964868\n",
      "Iteration 284, loss = 0.41950206\n",
      "Iteration 285, loss = 0.41940377\n",
      "Iteration 286, loss = 0.41918460\n",
      "Iteration 287, loss = 0.41911631\n",
      "Iteration 288, loss = 0.41876628\n",
      "Iteration 289, loss = 0.41880297\n",
      "Iteration 290, loss = 0.41874656\n",
      "Iteration 291, loss = 0.41881949\n",
      "Iteration 292, loss = 0.41876681\n",
      "Iteration 293, loss = 0.41856813\n",
      "Iteration 294, loss = 0.41836393\n",
      "Iteration 295, loss = 0.41828872\n",
      "Iteration 296, loss = 0.41822815\n",
      "Iteration 297, loss = 0.41782731\n",
      "Iteration 298, loss = 0.41782250\n",
      "Iteration 299, loss = 0.41769147\n",
      "Iteration 300, loss = 0.41759637\n",
      "Iteration 301, loss = 0.41745589\n",
      "Iteration 302, loss = 0.41737207\n",
      "Iteration 303, loss = 0.41721150\n",
      "Iteration 304, loss = 0.41716898\n",
      "Iteration 305, loss = 0.41703436\n",
      "Iteration 306, loss = 0.41699812\n",
      "Iteration 307, loss = 0.41685343\n",
      "Iteration 308, loss = 0.41672916\n",
      "Iteration 309, loss = 0.41677561\n",
      "Iteration 310, loss = 0.41665453\n",
      "Iteration 311, loss = 0.41665967\n",
      "Iteration 312, loss = 0.41652737\n",
      "Iteration 313, loss = 0.41647016\n",
      "Iteration 314, loss = 0.41636014\n",
      "Iteration 315, loss = 0.41639205\n",
      "Iteration 316, loss = 0.41647727\n",
      "Iteration 317, loss = 0.41653352\n",
      "Iteration 318, loss = 0.41662889\n",
      "Iteration 319, loss = 0.41667471\n",
      "Iteration 320, loss = 0.41646923\n",
      "Iteration 321, loss = 0.41618861\n",
      "Iteration 322, loss = 0.41595238\n",
      "Iteration 323, loss = 0.41595609\n",
      "Iteration 324, loss = 0.41574881\n",
      "Iteration 325, loss = 0.41557994\n",
      "Iteration 326, loss = 0.41579641\n",
      "Iteration 327, loss = 0.41573842\n",
      "Iteration 328, loss = 0.41568287\n",
      "Iteration 329, loss = 0.41542579\n",
      "Iteration 330, loss = 0.41521757\n",
      "Iteration 331, loss = 0.41536606\n",
      "Iteration 332, loss = 0.41532207\n",
      "Iteration 333, loss = 0.41526489\n",
      "Iteration 334, loss = 0.41505468\n",
      "Iteration 335, loss = 0.41497928\n",
      "Iteration 336, loss = 0.41484766\n",
      "Iteration 337, loss = 0.41455909\n",
      "Iteration 338, loss = 0.41462009\n",
      "Iteration 339, loss = 0.41450615\n",
      "Iteration 340, loss = 0.41437308\n",
      "Iteration 341, loss = 0.41429076\n",
      "Iteration 342, loss = 0.41412699\n",
      "Iteration 343, loss = 0.41404249\n",
      "Iteration 344, loss = 0.41400005\n",
      "Iteration 345, loss = 0.41396990\n",
      "Iteration 346, loss = 0.41380801\n",
      "Iteration 347, loss = 0.41369048\n",
      "Iteration 348, loss = 0.41359292\n",
      "Iteration 349, loss = 0.41348520\n",
      "Iteration 350, loss = 0.41341132\n",
      "Iteration 351, loss = 0.41315302\n",
      "Iteration 352, loss = 0.41342296\n",
      "Iteration 353, loss = 0.41331791\n",
      "Iteration 354, loss = 0.41322253\n",
      "Iteration 355, loss = 0.41327465\n",
      "Iteration 356, loss = 0.41316962\n",
      "Iteration 357, loss = 0.41300066\n",
      "Iteration 358, loss = 0.41287043\n",
      "Iteration 359, loss = 0.41284274\n",
      "Iteration 360, loss = 0.41265256\n",
      "Iteration 361, loss = 0.41255355\n",
      "Iteration 362, loss = 0.41237302\n",
      "Iteration 363, loss = 0.41219668\n",
      "Iteration 364, loss = 0.41204463\n",
      "Iteration 365, loss = 0.41194123\n",
      "Iteration 366, loss = 0.41178314\n",
      "Iteration 367, loss = 0.41172032\n",
      "Iteration 368, loss = 0.41153274\n",
      "Iteration 369, loss = 0.41154633\n",
      "Iteration 370, loss = 0.41142655\n",
      "Iteration 371, loss = 0.41142643\n",
      "Iteration 372, loss = 0.41124004\n",
      "Iteration 373, loss = 0.41112335\n",
      "Iteration 374, loss = 0.41095699\n",
      "Iteration 375, loss = 0.41100088\n",
      "Iteration 376, loss = 0.41101769\n",
      "Iteration 377, loss = 0.41089590\n",
      "Iteration 378, loss = 0.41084369\n",
      "Iteration 379, loss = 0.41072644\n",
      "Iteration 380, loss = 0.41089287\n",
      "Iteration 381, loss = 0.41111202\n",
      "Iteration 382, loss = 0.41106269\n",
      "Iteration 383, loss = 0.41088725\n",
      "Iteration 384, loss = 0.41078070\n",
      "Iteration 385, loss = 0.41074113\n",
      "Iteration 386, loss = 0.41059395\n",
      "Iteration 387, loss = 0.41031241\n",
      "Iteration 388, loss = 0.41012456\n",
      "Iteration 389, loss = 0.40978259\n",
      "Iteration 390, loss = 0.40974665\n",
      "Iteration 391, loss = 0.40983190\n",
      "Iteration 392, loss = 0.40963942\n",
      "Iteration 393, loss = 0.40958596\n",
      "Iteration 394, loss = 0.40955230\n",
      "Iteration 395, loss = 0.40970118\n",
      "Iteration 396, loss = 0.40961998\n",
      "Iteration 397, loss = 0.40963377\n",
      "Iteration 398, loss = 0.40958039\n",
      "Iteration 399, loss = 0.40952085\n",
      "Iteration 400, loss = 0.40940625\n",
      "Iteration 401, loss = 0.40914735\n",
      "Iteration 402, loss = 0.40891476\n",
      "Iteration 403, loss = 0.40863496\n",
      "Iteration 404, loss = 0.40881743\n",
      "Iteration 405, loss = 0.40873373\n",
      "Iteration 406, loss = 0.40865672\n",
      "Iteration 407, loss = 0.40885492\n",
      "Iteration 408, loss = 0.40877736\n",
      "Iteration 409, loss = 0.40860114\n",
      "Iteration 410, loss = 0.40852914\n",
      "Iteration 411, loss = 0.40821576\n",
      "Iteration 412, loss = 0.40823737\n",
      "Iteration 413, loss = 0.40817233\n",
      "Iteration 414, loss = 0.40793218\n",
      "Iteration 415, loss = 0.40794699\n",
      "Iteration 416, loss = 0.40777479\n",
      "Iteration 417, loss = 0.40765135\n",
      "Iteration 418, loss = 0.40763444\n",
      "Iteration 419, loss = 0.40756832\n",
      "Iteration 420, loss = 0.40748109\n",
      "Iteration 421, loss = 0.40742351\n",
      "Iteration 422, loss = 0.40740999\n",
      "Iteration 423, loss = 0.40732222\n",
      "Iteration 424, loss = 0.40740312\n",
      "Iteration 425, loss = 0.40734224\n",
      "Iteration 426, loss = 0.40719530\n",
      "Iteration 427, loss = 0.40720192\n",
      "Iteration 428, loss = 0.40709944\n",
      "Iteration 429, loss = 0.40709812\n",
      "Iteration 430, loss = 0.40694520\n",
      "Iteration 431, loss = 0.40685230\n",
      "Iteration 432, loss = 0.40670017\n",
      "Iteration 433, loss = 0.40674047\n",
      "Iteration 434, loss = 0.40661565\n",
      "Iteration 435, loss = 0.40655137\n",
      "Iteration 436, loss = 0.40640472\n",
      "Iteration 437, loss = 0.40636779\n",
      "Iteration 438, loss = 0.40629438\n",
      "Iteration 439, loss = 0.40625594\n",
      "Iteration 440, loss = 0.40630944\n",
      "Iteration 441, loss = 0.40632114\n",
      "Iteration 442, loss = 0.40619775\n",
      "Iteration 443, loss = 0.40604657\n",
      "Iteration 444, loss = 0.40592816\n",
      "Iteration 445, loss = 0.40585800\n",
      "Iteration 446, loss = 0.40577852\n",
      "Iteration 447, loss = 0.40579723\n",
      "Iteration 448, loss = 0.40576596\n",
      "Iteration 449, loss = 0.40548188\n",
      "Iteration 450, loss = 0.40527461\n",
      "Iteration 451, loss = 0.40524024\n",
      "Iteration 452, loss = 0.40518044\n",
      "Iteration 453, loss = 0.40510121\n",
      "Iteration 454, loss = 0.40491550\n",
      "Iteration 455, loss = 0.40486132\n",
      "Iteration 456, loss = 0.40470831\n",
      "Iteration 457, loss = 0.40458473\n",
      "Iteration 458, loss = 0.40488512\n",
      "Iteration 459, loss = 0.40484460\n",
      "Iteration 460, loss = 0.40474481\n",
      "Iteration 461, loss = 0.40455867\n",
      "Iteration 462, loss = 0.40453050\n",
      "Iteration 463, loss = 0.40444188\n",
      "Iteration 464, loss = 0.40431144\n",
      "Iteration 465, loss = 0.40440507\n",
      "Iteration 466, loss = 0.40462701\n",
      "Iteration 467, loss = 0.40482694\n",
      "Iteration 468, loss = 0.40485014\n",
      "Iteration 469, loss = 0.40492162\n",
      "Iteration 470, loss = 0.40482534\n",
      "Iteration 471, loss = 0.40482765\n",
      "Iteration 472, loss = 0.40487405\n",
      "Iteration 473, loss = 0.40469136\n",
      "Iteration 474, loss = 0.40438546\n",
      "Iteration 475, loss = 0.40412650\n",
      "Iteration 476, loss = 0.40376557\n",
      "Iteration 477, loss = 0.40351452\n",
      "Iteration 478, loss = 0.40331320\n",
      "Iteration 479, loss = 0.40329392\n",
      "Iteration 480, loss = 0.40315874\n",
      "Iteration 481, loss = 0.40298958\n",
      "Iteration 482, loss = 0.40281953\n",
      "Iteration 483, loss = 0.40287012\n",
      "Iteration 484, loss = 0.40270981\n",
      "Iteration 485, loss = 0.40278549\n",
      "Iteration 486, loss = 0.40277288\n",
      "Iteration 487, loss = 0.40275954\n",
      "Iteration 488, loss = 0.40257823\n",
      "Iteration 489, loss = 0.40254717\n",
      "Iteration 490, loss = 0.40248703\n",
      "Iteration 491, loss = 0.40229471\n",
      "Iteration 492, loss = 0.40232410\n",
      "Iteration 493, loss = 0.40229261\n",
      "Iteration 494, loss = 0.40213173\n",
      "Iteration 495, loss = 0.40222757\n",
      "Iteration 496, loss = 0.40235629\n",
      "Iteration 497, loss = 0.40244948\n",
      "Iteration 498, loss = 0.40246968\n",
      "Iteration 499, loss = 0.40252825\n",
      "Iteration 500, loss = 0.40268474\n",
      "Iteration 501, loss = 0.40260301\n",
      "Iteration 502, loss = 0.40251732\n",
      "Iteration 503, loss = 0.40221177\n",
      "Iteration 504, loss = 0.40193920\n",
      "Iteration 505, loss = 0.40185768\n",
      "Iteration 506, loss = 0.40172413\n",
      "Iteration 507, loss = 0.40152393\n",
      "Iteration 508, loss = 0.40140282\n",
      "Iteration 509, loss = 0.40135007\n",
      "Iteration 510, loss = 0.40122134\n",
      "Iteration 511, loss = 0.40121100\n",
      "Iteration 512, loss = 0.40106416\n",
      "Iteration 513, loss = 0.40081707\n",
      "Iteration 514, loss = 0.40076100\n",
      "Iteration 515, loss = 0.40083040\n",
      "Iteration 516, loss = 0.40054531\n",
      "Iteration 517, loss = 0.40074753\n",
      "Iteration 518, loss = 0.40077943\n",
      "Iteration 519, loss = 0.40093016\n",
      "Iteration 520, loss = 0.40124801\n",
      "Iteration 521, loss = 0.40127548\n",
      "Iteration 522, loss = 0.40113882\n",
      "Iteration 523, loss = 0.40063236\n",
      "Iteration 524, loss = 0.40046163\n",
      "Iteration 525, loss = 0.40012972\n",
      "Iteration 526, loss = 0.40013586\n",
      "Iteration 527, loss = 0.39976834\n",
      "Iteration 528, loss = 0.39966962\n",
      "Iteration 529, loss = 0.39950966\n",
      "Iteration 530, loss = 0.39947433\n",
      "Iteration 531, loss = 0.39944112\n",
      "Iteration 532, loss = 0.39924710\n",
      "Iteration 533, loss = 0.39934109\n",
      "Iteration 534, loss = 0.39940853\n",
      "Iteration 535, loss = 0.39929564\n",
      "Iteration 536, loss = 0.39924161\n",
      "Iteration 537, loss = 0.39905006\n",
      "Iteration 538, loss = 0.39904933\n",
      "Iteration 539, loss = 0.39896568\n",
      "Iteration 540, loss = 0.39869182\n",
      "Iteration 541, loss = 0.39853067\n",
      "Iteration 542, loss = 0.39857709\n",
      "Iteration 543, loss = 0.39854842\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 544, loss = 0.39848546\n",
      "Iteration 545, loss = 0.39843060\n",
      "Iteration 546, loss = 0.39831570\n",
      "Iteration 547, loss = 0.39818643\n",
      "Iteration 548, loss = 0.39791231\n",
      "Iteration 549, loss = 0.39773423\n",
      "Iteration 550, loss = 0.39761827\n",
      "Iteration 551, loss = 0.39766228\n",
      "Iteration 552, loss = 0.39792787\n",
      "Iteration 553, loss = 0.39881909\n",
      "Iteration 554, loss = 0.39961156\n",
      "Iteration 555, loss = 0.40011696\n",
      "Iteration 556, loss = 0.40052895\n",
      "Iteration 557, loss = 0.40039409\n",
      "Iteration 558, loss = 0.39988037\n",
      "Iteration 559, loss = 0.39932177\n",
      "Iteration 560, loss = 0.39870111\n",
      "Iteration 561, loss = 0.39804548\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(10, 10), max_iter=1000, verbose=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier  \n",
    "mlp = MLPClassifier(hidden_layer_sizes=(10,10), max_iter=1000, verbose=2)  \n",
    "mlp.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7337662337662337\n",
      "[[82 18]\n",
      " [23 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.82      0.80       100\n",
      "         1.0       0.63      0.57      0.60        54\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.71      0.70      0.70       154\n",
      "weighted avg       0.73      0.73      0.73       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = mlp.predict(X_test)  \n",
    "print(\"Accuracy\", metrics.accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test,predictions))  \n",
    "print(classification_report(y_test,predictions))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.75160496\n",
      "Iteration 2, loss = 0.74138509\n",
      "Iteration 3, loss = 0.73176955\n",
      "Iteration 4, loss = 0.72287057\n",
      "Iteration 5, loss = 0.71383056\n",
      "Iteration 6, loss = 0.70562093\n",
      "Iteration 7, loss = 0.69729636\n",
      "Iteration 8, loss = 0.68996986\n",
      "Iteration 9, loss = 0.68242848\n",
      "Iteration 10, loss = 0.67521510\n",
      "Iteration 11, loss = 0.66828426\n",
      "Iteration 12, loss = 0.66194984\n",
      "Iteration 13, loss = 0.65563461\n",
      "Iteration 14, loss = 0.64982526\n",
      "Iteration 15, loss = 0.64418005\n",
      "Iteration 16, loss = 0.63872678\n",
      "Iteration 17, loss = 0.63354889\n",
      "Iteration 18, loss = 0.62829862\n",
      "Iteration 19, loss = 0.62325108\n",
      "Iteration 20, loss = 0.61832082\n",
      "Iteration 21, loss = 0.61398545\n",
      "Iteration 22, loss = 0.60949417\n",
      "Iteration 23, loss = 0.60571999\n",
      "Iteration 24, loss = 0.60204388\n",
      "Iteration 25, loss = 0.59853230\n",
      "Iteration 26, loss = 0.59504359\n",
      "Iteration 27, loss = 0.59168439\n",
      "Iteration 28, loss = 0.58850056\n",
      "Iteration 29, loss = 0.58508157\n",
      "Iteration 30, loss = 0.58196502\n",
      "Iteration 31, loss = 0.57859038\n",
      "Iteration 32, loss = 0.57563992\n",
      "Iteration 33, loss = 0.57301262\n",
      "Iteration 34, loss = 0.57021362\n",
      "Iteration 35, loss = 0.56760176\n",
      "Iteration 36, loss = 0.56527216\n",
      "Iteration 37, loss = 0.56281352\n",
      "Iteration 38, loss = 0.56092115\n",
      "Iteration 39, loss = 0.55877847\n",
      "Iteration 40, loss = 0.55682791\n",
      "Iteration 41, loss = 0.55494537\n",
      "Iteration 42, loss = 0.55309161\n",
      "Iteration 43, loss = 0.55139675\n",
      "Iteration 44, loss = 0.54962345\n",
      "Iteration 45, loss = 0.54787171\n",
      "Iteration 46, loss = 0.54669364\n",
      "Iteration 47, loss = 0.54557543\n",
      "Iteration 48, loss = 0.54455616\n",
      "Iteration 49, loss = 0.54325359\n",
      "Iteration 50, loss = 0.54204124\n",
      "Iteration 51, loss = 0.54074995\n",
      "Iteration 52, loss = 0.53954273\n",
      "Iteration 53, loss = 0.53848045\n",
      "Iteration 54, loss = 0.53730532\n",
      "Iteration 55, loss = 0.53608019\n",
      "Iteration 56, loss = 0.53489155\n",
      "Iteration 57, loss = 0.53380079\n",
      "Iteration 58, loss = 0.53260623\n",
      "Iteration 59, loss = 0.53148523\n",
      "Iteration 60, loss = 0.53035784\n",
      "Iteration 61, loss = 0.52934218\n",
      "Iteration 62, loss = 0.52813460\n",
      "Iteration 63, loss = 0.52706794\n",
      "Iteration 64, loss = 0.52613036\n",
      "Iteration 65, loss = 0.52503655\n",
      "Iteration 66, loss = 0.52380975\n",
      "Iteration 67, loss = 0.52284152\n",
      "Iteration 68, loss = 0.52188493\n",
      "Iteration 69, loss = 0.52086371\n",
      "Iteration 70, loss = 0.51990117\n",
      "Iteration 71, loss = 0.51903548\n",
      "Iteration 72, loss = 0.51812989\n",
      "Iteration 73, loss = 0.51730723\n",
      "Iteration 74, loss = 0.51654855\n",
      "Iteration 75, loss = 0.51573170\n",
      "Iteration 76, loss = 0.51496393\n",
      "Iteration 77, loss = 0.51411202\n",
      "Iteration 78, loss = 0.51344854\n",
      "Iteration 79, loss = 0.51272533\n",
      "Iteration 80, loss = 0.51196363\n",
      "Iteration 81, loss = 0.51123026\n",
      "Iteration 82, loss = 0.51051135\n",
      "Iteration 83, loss = 0.50974652\n",
      "Iteration 84, loss = 0.50900570\n",
      "Iteration 85, loss = 0.50813760\n",
      "Iteration 86, loss = 0.50742774\n",
      "Iteration 87, loss = 0.50663817\n",
      "Iteration 88, loss = 0.50591732\n",
      "Iteration 89, loss = 0.50519461\n",
      "Iteration 90, loss = 0.50448173\n",
      "Iteration 91, loss = 0.50394481\n",
      "Iteration 92, loss = 0.50344140\n",
      "Iteration 93, loss = 0.50299395\n",
      "Iteration 94, loss = 0.50255079\n",
      "Iteration 95, loss = 0.50183716\n",
      "Iteration 96, loss = 0.50127051\n",
      "Iteration 97, loss = 0.50069264\n",
      "Iteration 98, loss = 0.50007444\n",
      "Iteration 99, loss = 0.49967936\n",
      "Iteration 100, loss = 0.49904129\n",
      "Iteration 101, loss = 0.49837419\n",
      "Iteration 102, loss = 0.49772726\n",
      "Iteration 103, loss = 0.49713850\n",
      "Iteration 104, loss = 0.49645517\n",
      "Iteration 105, loss = 0.49587805\n",
      "Iteration 106, loss = 0.49523795\n",
      "Iteration 107, loss = 0.49472359\n",
      "Iteration 108, loss = 0.49432107\n",
      "Iteration 109, loss = 0.49383777\n",
      "Iteration 110, loss = 0.49344044\n",
      "Iteration 111, loss = 0.49300224\n",
      "Iteration 112, loss = 0.49266375\n",
      "Iteration 113, loss = 0.49231368\n",
      "Iteration 114, loss = 0.49188946\n",
      "Iteration 115, loss = 0.49176427\n",
      "Iteration 116, loss = 0.49129093\n",
      "Iteration 117, loss = 0.49083883\n",
      "Iteration 118, loss = 0.49021956\n",
      "Iteration 119, loss = 0.48962987\n",
      "Iteration 120, loss = 0.48918254\n",
      "Iteration 121, loss = 0.48854349\n",
      "Iteration 122, loss = 0.48808553\n",
      "Iteration 123, loss = 0.48762283\n",
      "Iteration 124, loss = 0.48723220\n",
      "Iteration 125, loss = 0.48689188\n",
      "Iteration 126, loss = 0.48655097\n",
      "Iteration 127, loss = 0.48618643\n",
      "Iteration 128, loss = 0.48587143\n",
      "Iteration 129, loss = 0.48555617\n",
      "Iteration 130, loss = 0.48523931\n",
      "Iteration 131, loss = 0.48483516\n",
      "Iteration 132, loss = 0.48448521\n",
      "Iteration 133, loss = 0.48404907\n",
      "Iteration 134, loss = 0.48374725\n",
      "Iteration 135, loss = 0.48338902\n",
      "Iteration 136, loss = 0.48312128\n",
      "Iteration 137, loss = 0.48262471\n",
      "Iteration 138, loss = 0.48211452\n",
      "Iteration 139, loss = 0.48171520\n",
      "Iteration 140, loss = 0.48157446\n",
      "Iteration 141, loss = 0.48104680\n",
      "Iteration 142, loss = 0.48079994\n",
      "Iteration 143, loss = 0.48075207\n",
      "Iteration 144, loss = 0.48034560\n",
      "Iteration 145, loss = 0.47991502\n",
      "Iteration 146, loss = 0.47944435\n",
      "Iteration 147, loss = 0.47909006\n",
      "Iteration 148, loss = 0.47871795\n",
      "Iteration 149, loss = 0.47834619\n",
      "Iteration 150, loss = 0.47789742\n",
      "Iteration 151, loss = 0.47758407\n",
      "Iteration 152, loss = 0.47723667\n",
      "Iteration 153, loss = 0.47700144\n",
      "Iteration 154, loss = 0.47677643\n",
      "Iteration 155, loss = 0.47650705\n",
      "Iteration 156, loss = 0.47644316\n",
      "Iteration 157, loss = 0.47623802\n",
      "Iteration 158, loss = 0.47616102\n",
      "Iteration 159, loss = 0.47586056\n",
      "Iteration 160, loss = 0.47536016\n",
      "Iteration 161, loss = 0.47484658\n",
      "Iteration 162, loss = 0.47428216\n",
      "Iteration 163, loss = 0.47386326\n",
      "Iteration 164, loss = 0.47346809\n",
      "Iteration 165, loss = 0.47301856\n",
      "Iteration 166, loss = 0.47252937\n",
      "Iteration 167, loss = 0.47214488\n",
      "Iteration 168, loss = 0.47195882\n",
      "Iteration 169, loss = 0.47168262\n",
      "Iteration 170, loss = 0.47131600\n",
      "Iteration 171, loss = 0.47091580\n",
      "Iteration 172, loss = 0.47045664\n",
      "Iteration 173, loss = 0.47007085\n",
      "Iteration 174, loss = 0.46962318\n",
      "Iteration 175, loss = 0.46918193\n",
      "Iteration 176, loss = 0.46877190\n",
      "Iteration 177, loss = 0.46840845\n",
      "Iteration 178, loss = 0.46803062\n",
      "Iteration 179, loss = 0.46770741\n",
      "Iteration 180, loss = 0.46738439\n",
      "Iteration 181, loss = 0.46707350\n",
      "Iteration 182, loss = 0.46667498\n",
      "Iteration 183, loss = 0.46643347\n",
      "Iteration 184, loss = 0.46612239\n",
      "Iteration 185, loss = 0.46585388\n",
      "Iteration 186, loss = 0.46556178\n",
      "Iteration 187, loss = 0.46535513\n",
      "Iteration 188, loss = 0.46502056\n",
      "Iteration 189, loss = 0.46492555\n",
      "Iteration 190, loss = 0.46454061\n",
      "Iteration 191, loss = 0.46433470\n",
      "Iteration 192, loss = 0.46423222\n",
      "Iteration 193, loss = 0.46410926\n",
      "Iteration 194, loss = 0.46386657\n",
      "Iteration 195, loss = 0.46359568\n",
      "Iteration 196, loss = 0.46348856\n",
      "Iteration 197, loss = 0.46327851\n",
      "Iteration 198, loss = 0.46313766\n",
      "Iteration 199, loss = 0.46309044\n",
      "Iteration 200, loss = 0.46302310\n",
      "Iteration 201, loss = 0.46297442\n",
      "Iteration 202, loss = 0.46284682\n",
      "Iteration 203, loss = 0.46271476\n",
      "Iteration 204, loss = 0.46255796\n",
      "Iteration 205, loss = 0.46248465\n",
      "Iteration 206, loss = 0.46212788\n",
      "Iteration 207, loss = 0.46187014\n",
      "Iteration 208, loss = 0.46153219\n",
      "Iteration 209, loss = 0.46129949\n",
      "Iteration 210, loss = 0.46112599\n",
      "Iteration 211, loss = 0.46089327\n",
      "Iteration 212, loss = 0.46071154\n",
      "Iteration 213, loss = 0.46049784\n",
      "Iteration 214, loss = 0.46018787\n",
      "Iteration 215, loss = 0.46001559\n",
      "Iteration 216, loss = 0.45973615\n",
      "Iteration 217, loss = 0.45948225\n",
      "Iteration 218, loss = 0.45934258\n",
      "Iteration 219, loss = 0.45915783\n",
      "Iteration 220, loss = 0.45902127\n",
      "Iteration 221, loss = 0.45883490\n",
      "Iteration 222, loss = 0.45866503\n",
      "Iteration 223, loss = 0.45853830\n",
      "Iteration 224, loss = 0.45842260\n",
      "Iteration 225, loss = 0.45830238\n",
      "Iteration 226, loss = 0.45810202\n",
      "Iteration 227, loss = 0.45793476\n",
      "Iteration 228, loss = 0.45779144\n",
      "Iteration 229, loss = 0.45780551\n",
      "Iteration 230, loss = 0.45748478\n",
      "Iteration 231, loss = 0.45740713\n",
      "Iteration 232, loss = 0.45726285\n",
      "Iteration 233, loss = 0.45713768\n",
      "Iteration 234, loss = 0.45708746\n",
      "Iteration 235, loss = 0.45694673\n",
      "Iteration 236, loss = 0.45685469\n",
      "Iteration 237, loss = 0.45673402\n",
      "Iteration 238, loss = 0.45656288\n",
      "Iteration 239, loss = 0.45637061\n",
      "Iteration 240, loss = 0.45617917\n",
      "Iteration 241, loss = 0.45612745\n",
      "Iteration 242, loss = 0.45592540\n",
      "Iteration 243, loss = 0.45581785\n",
      "Iteration 244, loss = 0.45572229\n",
      "Iteration 245, loss = 0.45561575\n",
      "Iteration 246, loss = 0.45546528\n",
      "Iteration 247, loss = 0.45536335\n",
      "Iteration 248, loss = 0.45529215\n",
      "Iteration 249, loss = 0.45508689\n",
      "Iteration 250, loss = 0.45487480\n",
      "Iteration 251, loss = 0.45461964\n",
      "Iteration 252, loss = 0.45458252\n",
      "Iteration 253, loss = 0.45431977\n",
      "Iteration 254, loss = 0.45421280\n",
      "Iteration 255, loss = 0.45406108\n",
      "Iteration 256, loss = 0.45373349\n",
      "Iteration 257, loss = 0.45378108\n",
      "Iteration 258, loss = 0.45352595\n",
      "Iteration 259, loss = 0.45343799\n",
      "Iteration 260, loss = 0.45339622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 261, loss = 0.45333475\n",
      "Iteration 262, loss = 0.45317341\n",
      "Iteration 263, loss = 0.45300504\n",
      "Iteration 264, loss = 0.45286484\n",
      "Iteration 265, loss = 0.45279511\n",
      "Iteration 266, loss = 0.45273260\n",
      "Iteration 267, loss = 0.45246613\n",
      "Iteration 268, loss = 0.45242296\n",
      "Iteration 269, loss = 0.45231226\n",
      "Iteration 270, loss = 0.45219769\n",
      "Iteration 271, loss = 0.45212082\n",
      "Iteration 272, loss = 0.45210759\n",
      "Iteration 273, loss = 0.45195093\n",
      "Iteration 274, loss = 0.45182620\n",
      "Iteration 275, loss = 0.45177450\n",
      "Iteration 276, loss = 0.45170838\n",
      "Iteration 277, loss = 0.45153971\n",
      "Iteration 278, loss = 0.45138761\n",
      "Iteration 279, loss = 0.45120369\n",
      "Iteration 280, loss = 0.45111618\n",
      "Iteration 281, loss = 0.45094137\n",
      "Iteration 282, loss = 0.45079792\n",
      "Iteration 283, loss = 0.45074481\n",
      "Iteration 284, loss = 0.45065423\n",
      "Iteration 285, loss = 0.45065416\n",
      "Iteration 286, loss = 0.45061721\n",
      "Iteration 287, loss = 0.45041776\n",
      "Iteration 288, loss = 0.45019468\n",
      "Iteration 289, loss = 0.45009072\n",
      "Iteration 290, loss = 0.44998639\n",
      "Iteration 291, loss = 0.44988700\n",
      "Iteration 292, loss = 0.44979289\n",
      "Iteration 293, loss = 0.44962808\n",
      "Iteration 294, loss = 0.44954224\n",
      "Iteration 295, loss = 0.44939176\n",
      "Iteration 296, loss = 0.44933332\n",
      "Iteration 297, loss = 0.44924288\n",
      "Iteration 298, loss = 0.44915217\n",
      "Iteration 299, loss = 0.44909509\n",
      "Iteration 300, loss = 0.44900858\n",
      "Iteration 301, loss = 0.44884263\n",
      "Iteration 302, loss = 0.44874329\n",
      "Iteration 303, loss = 0.44866587\n",
      "Iteration 304, loss = 0.44857794\n",
      "Iteration 305, loss = 0.44848060\n",
      "Iteration 306, loss = 0.44832619\n",
      "Iteration 307, loss = 0.44819279\n",
      "Iteration 308, loss = 0.44808562\n",
      "Iteration 309, loss = 0.44795060\n",
      "Iteration 310, loss = 0.44786405\n",
      "Iteration 311, loss = 0.44773409\n",
      "Iteration 312, loss = 0.44778340\n",
      "Iteration 313, loss = 0.44768548\n",
      "Iteration 314, loss = 0.44764074\n",
      "Iteration 315, loss = 0.44756160\n",
      "Iteration 316, loss = 0.44750160\n",
      "Iteration 317, loss = 0.44744664\n",
      "Iteration 318, loss = 0.44739335\n",
      "Iteration 319, loss = 0.44725379\n",
      "Iteration 320, loss = 0.44713307\n",
      "Iteration 321, loss = 0.44697345\n",
      "Iteration 322, loss = 0.44684578\n",
      "Iteration 323, loss = 0.44678429\n",
      "Iteration 324, loss = 0.44668334\n",
      "Iteration 325, loss = 0.44658109\n",
      "Iteration 326, loss = 0.44647355\n",
      "Iteration 327, loss = 0.44639551\n",
      "Iteration 328, loss = 0.44631453\n",
      "Iteration 329, loss = 0.44621327\n",
      "Iteration 330, loss = 0.44622607\n",
      "Iteration 331, loss = 0.44613827\n",
      "Iteration 332, loss = 0.44601258\n",
      "Iteration 333, loss = 0.44599286\n",
      "Iteration 334, loss = 0.44586904\n",
      "Iteration 335, loss = 0.44575708\n",
      "Iteration 336, loss = 0.44562517\n",
      "Iteration 337, loss = 0.44554981\n",
      "Iteration 338, loss = 0.44543969\n",
      "Iteration 339, loss = 0.44534083\n",
      "Iteration 340, loss = 0.44519280\n",
      "Iteration 341, loss = 0.44527160\n",
      "Iteration 342, loss = 0.44510393\n",
      "Iteration 343, loss = 0.44509072\n",
      "Iteration 344, loss = 0.44502455\n",
      "Iteration 345, loss = 0.44500643\n",
      "Iteration 346, loss = 0.44489928\n",
      "Iteration 347, loss = 0.44475137\n",
      "Iteration 348, loss = 0.44475049\n",
      "Iteration 349, loss = 0.44471311\n",
      "Iteration 350, loss = 0.44465999\n",
      "Iteration 351, loss = 0.44457185\n",
      "Iteration 352, loss = 0.44448434\n",
      "Iteration 353, loss = 0.44432650\n",
      "Iteration 354, loss = 0.44419576\n",
      "Iteration 355, loss = 0.44416551\n",
      "Iteration 356, loss = 0.44408544\n",
      "Iteration 357, loss = 0.44407642\n",
      "Iteration 358, loss = 0.44401084\n",
      "Iteration 359, loss = 0.44394972\n",
      "Iteration 360, loss = 0.44389935\n",
      "Iteration 361, loss = 0.44388583\n",
      "Iteration 362, loss = 0.44387144\n",
      "Iteration 363, loss = 0.44373341\n",
      "Iteration 364, loss = 0.44366243\n",
      "Iteration 365, loss = 0.44360409\n",
      "Iteration 366, loss = 0.44349838\n",
      "Iteration 367, loss = 0.44345538\n",
      "Iteration 368, loss = 0.44335675\n",
      "Iteration 369, loss = 0.44336880\n",
      "Iteration 370, loss = 0.44328560\n",
      "Iteration 371, loss = 0.44326809\n",
      "Iteration 372, loss = 0.44321030\n",
      "Iteration 373, loss = 0.44321068\n",
      "Iteration 374, loss = 0.44317174\n",
      "Iteration 375, loss = 0.44325376\n",
      "Iteration 376, loss = 0.44323731\n",
      "Iteration 377, loss = 0.44314409\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(5, 5), max_iter=1000, verbose=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp2 = MLPClassifier(hidden_layer_sizes=(5,5), max_iter=1000, verbose=2)  \n",
    "mlp2.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7272727272727273\n",
      "[[81 19]\n",
      " [23 31]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.78      0.81      0.79       100\n",
      "         1.0       0.62      0.57      0.60        54\n",
      "\n",
      "    accuracy                           0.73       154\n",
      "   macro avg       0.70      0.69      0.70       154\n",
      "weighted avg       0.72      0.73      0.72       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = mlp2.predict(X_test)  \n",
    "print(\"Accuracy\", metrics.accuracy_score(y_test, pred))\n",
    "print(confusion_matrix(y_test,pred))  \n",
    "print(classification_report(y_test,pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.plot(mlp.loss_curve_)\n",
    "plt.plot(mlp2.loss_curve_)\n",
    "plt.title(\"NN Loss Curves\")\n",
    "plt.legend(['NN','NN2'],loc =\"right\")\n",
    "plt.xlabel(\"number of steps\")\n",
    "plt.ylabel(\"loss function\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 12)                108       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 104       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 221\n",
      "Trainable params: 221\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "# create keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(8, kernel_initializer='uniform', activation='relu'))\n",
    "model.add(Dense(1, kernel_initializer='uniform', activation='sigmoid'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\anaconda\\envs\\anirban\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From C:\\anaconda\\envs\\anirban\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 460 samples, validate on 154 samples\n",
      "Epoch 1/60\n",
      "460/460 [==============================] - 0s 989us/step - loss: 0.6779 - accuracy: 0.6478 - val_loss: 0.6579 - val_accuracy: 0.6364\n",
      "Epoch 2/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.6304 - accuracy: 0.6522 - val_loss: 0.5976 - val_accuracy: 0.6429\n",
      "Epoch 3/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.5767 - accuracy: 0.6935 - val_loss: 0.5523 - val_accuracy: 0.7468\n",
      "Epoch 4/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.5415 - accuracy: 0.7630 - val_loss: 0.5249 - val_accuracy: 0.7597\n",
      "Epoch 5/60\n",
      "460/460 [==============================] - 0s 197us/step - loss: 0.5204 - accuracy: 0.7761 - val_loss: 0.5086 - val_accuracy: 0.7662\n",
      "Epoch 6/60\n",
      "460/460 [==============================] - 0s 204us/step - loss: 0.5069 - accuracy: 0.7804 - val_loss: 0.4967 - val_accuracy: 0.7597\n",
      "Epoch 7/60\n",
      "460/460 [==============================] - 0s 204us/step - loss: 0.4975 - accuracy: 0.7870 - val_loss: 0.4887 - val_accuracy: 0.7597\n",
      "Epoch 8/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4902 - accuracy: 0.7913 - val_loss: 0.4817 - val_accuracy: 0.7662\n",
      "Epoch 9/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.4849 - accuracy: 0.7870 - val_loss: 0.4771 - val_accuracy: 0.7792\n",
      "Epoch 10/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4804 - accuracy: 0.7891 - val_loss: 0.4727 - val_accuracy: 0.7857\n",
      "Epoch 11/60\n",
      "460/460 [==============================] - 0s 199us/step - loss: 0.4765 - accuracy: 0.7913 - val_loss: 0.4691 - val_accuracy: 0.7922\n",
      "Epoch 12/60\n",
      "460/460 [==============================] - 0s 202us/step - loss: 0.4738 - accuracy: 0.7957 - val_loss: 0.4664 - val_accuracy: 0.7922\n",
      "Epoch 13/60\n",
      "460/460 [==============================] - 0s 195us/step - loss: 0.4709 - accuracy: 0.7913 - val_loss: 0.4634 - val_accuracy: 0.7857\n",
      "Epoch 14/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.4685 - accuracy: 0.7957 - val_loss: 0.4617 - val_accuracy: 0.7857\n",
      "Epoch 15/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4668 - accuracy: 0.7913 - val_loss: 0.4605 - val_accuracy: 0.7857\n",
      "Epoch 16/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4654 - accuracy: 0.7935 - val_loss: 0.4592 - val_accuracy: 0.7857\n",
      "Epoch 17/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4642 - accuracy: 0.7978 - val_loss: 0.4578 - val_accuracy: 0.7857\n",
      "Epoch 18/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4627 - accuracy: 0.7935 - val_loss: 0.4569 - val_accuracy: 0.7857\n",
      "Epoch 19/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4616 - accuracy: 0.7913 - val_loss: 0.4552 - val_accuracy: 0.7922\n",
      "Epoch 20/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4608 - accuracy: 0.7935 - val_loss: 0.4545 - val_accuracy: 0.7922\n",
      "Epoch 21/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4601 - accuracy: 0.7935 - val_loss: 0.4537 - val_accuracy: 0.7922\n",
      "Epoch 22/60\n",
      "460/460 [==============================] - 0s 215us/step - loss: 0.4593 - accuracy: 0.7870 - val_loss: 0.4530 - val_accuracy: 0.7922\n",
      "Epoch 23/60\n",
      "460/460 [==============================] - 0s 215us/step - loss: 0.4586 - accuracy: 0.7870 - val_loss: 0.4523 - val_accuracy: 0.7922\n",
      "Epoch 24/60\n",
      "460/460 [==============================] - 0s 215us/step - loss: 0.4579 - accuracy: 0.7891 - val_loss: 0.4516 - val_accuracy: 0.7857\n",
      "Epoch 25/60\n",
      "460/460 [==============================] - 0s 204us/step - loss: 0.4575 - accuracy: 0.7913 - val_loss: 0.4515 - val_accuracy: 0.7922\n",
      "Epoch 26/60\n",
      "460/460 [==============================] - 0s 204us/step - loss: 0.4566 - accuracy: 0.7891 - val_loss: 0.4510 - val_accuracy: 0.7922\n",
      "Epoch 27/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4562 - accuracy: 0.7848 - val_loss: 0.4509 - val_accuracy: 0.7792\n",
      "Epoch 28/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4557 - accuracy: 0.7848 - val_loss: 0.4508 - val_accuracy: 0.7792\n",
      "Epoch 29/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.4552 - accuracy: 0.7848 - val_loss: 0.4503 - val_accuracy: 0.7792\n",
      "Epoch 30/60\n",
      "460/460 [==============================] - 0s 204us/step - loss: 0.4549 - accuracy: 0.7848 - val_loss: 0.4498 - val_accuracy: 0.7857\n",
      "Epoch 31/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4545 - accuracy: 0.7870 - val_loss: 0.4497 - val_accuracy: 0.7857\n",
      "Epoch 32/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4541 - accuracy: 0.7848 - val_loss: 0.4492 - val_accuracy: 0.7857\n",
      "Epoch 33/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4537 - accuracy: 0.7848 - val_loss: 0.4498 - val_accuracy: 0.7857\n",
      "Epoch 34/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4533 - accuracy: 0.7891 - val_loss: 0.4493 - val_accuracy: 0.7792\n",
      "Epoch 35/60\n",
      "460/460 [==============================] - 0s 215us/step - loss: 0.4530 - accuracy: 0.7870 - val_loss: 0.4495 - val_accuracy: 0.7857\n",
      "Epoch 36/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4527 - accuracy: 0.7848 - val_loss: 0.4496 - val_accuracy: 0.7792\n",
      "Epoch 37/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4524 - accuracy: 0.7891 - val_loss: 0.4497 - val_accuracy: 0.7792\n",
      "Epoch 38/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4522 - accuracy: 0.7870 - val_loss: 0.4496 - val_accuracy: 0.7792\n",
      "Epoch 39/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4519 - accuracy: 0.7870 - val_loss: 0.4491 - val_accuracy: 0.7792\n",
      "Epoch 40/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.4516 - accuracy: 0.7870 - val_loss: 0.4490 - val_accuracy: 0.7792\n",
      "Epoch 41/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4513 - accuracy: 0.7848 - val_loss: 0.4488 - val_accuracy: 0.7792\n",
      "Epoch 42/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4511 - accuracy: 0.7870 - val_loss: 0.4488 - val_accuracy: 0.7792\n",
      "Epoch 43/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4510 - accuracy: 0.7870 - val_loss: 0.4488 - val_accuracy: 0.7792\n",
      "Epoch 44/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4506 - accuracy: 0.7870 - val_loss: 0.4490 - val_accuracy: 0.7792\n",
      "Epoch 45/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4505 - accuracy: 0.7891 - val_loss: 0.4493 - val_accuracy: 0.7792\n",
      "Epoch 46/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4502 - accuracy: 0.7891 - val_loss: 0.4493 - val_accuracy: 0.7792\n",
      "Epoch 47/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4500 - accuracy: 0.7891 - val_loss: 0.4493 - val_accuracy: 0.7792\n",
      "Epoch 48/60\n",
      "460/460 [==============================] - 0s 212us/step - loss: 0.4499 - accuracy: 0.7891 - val_loss: 0.4494 - val_accuracy: 0.7792\n",
      "Epoch 49/60\n",
      "460/460 [==============================] - 0s 206us/step - loss: 0.4496 - accuracy: 0.7891 - val_loss: 0.4490 - val_accuracy: 0.7792\n",
      "Epoch 50/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4494 - accuracy: 0.7870 - val_loss: 0.4486 - val_accuracy: 0.7792\n",
      "Epoch 51/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4492 - accuracy: 0.7870 - val_loss: 0.4484 - val_accuracy: 0.7792\n",
      "Epoch 52/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4491 - accuracy: 0.7848 - val_loss: 0.4486 - val_accuracy: 0.7792\n",
      "Epoch 53/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4488 - accuracy: 0.7870 - val_loss: 0.4485 - val_accuracy: 0.7792\n",
      "Epoch 54/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4486 - accuracy: 0.7870 - val_loss: 0.4483 - val_accuracy: 0.7792\n",
      "Epoch 55/60\n",
      "460/460 [==============================] - 0s 221us/step - loss: 0.4484 - accuracy: 0.7848 - val_loss: 0.4484 - val_accuracy: 0.7792\n",
      "Epoch 56/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4483 - accuracy: 0.7870 - val_loss: 0.4482 - val_accuracy: 0.7792\n",
      "Epoch 57/60\n",
      "460/460 [==============================] - 0s 210us/step - loss: 0.4481 - accuracy: 0.7848 - val_loss: 0.4480 - val_accuracy: 0.7792\n",
      "Epoch 58/60\n",
      "460/460 [==============================] - 0s 208us/step - loss: 0.4480 - accuracy: 0.7848 - val_loss: 0.4482 - val_accuracy: 0.7792\n",
      "Epoch 59/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4477 - accuracy: 0.7848 - val_loss: 0.4480 - val_accuracy: 0.7792\n",
      "Epoch 60/60\n",
      "460/460 [==============================] - 0s 217us/step - loss: 0.4476 - accuracy: 0.7848 - val_loss: 0.4477 - val_accuracy: 0.7792\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adagrad', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "history=model.fit(X_train, y_train, validation_split=0.25, epochs=60, batch_size=12)\n",
    "# calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 0.10116405,  0.06283816, -0.27731368,  0.0579188 ,  0.04015678,\n",
      "         0.18729658, -0.05724917,  0.0578138 , -0.18630266, -0.24949664,\n",
      "         0.13852213, -0.19038981],\n",
      "       [ 0.3786893 ,  0.45312858, -0.24126463,  0.41020316, -0.23745908,\n",
      "         0.4127419 ,  0.41623348,  0.48954907, -0.2776172 , -0.28236252,\n",
      "         0.41035298, -0.3100145 ],\n",
      "       [-0.11492144, -0.09823404,  0.00658755, -0.0533898 ,  0.00323889,\n",
      "        -0.12547438, -0.04313909,  0.01604349,  0.06138343,  0.04399497,\n",
      "        -0.11627509,  0.03330745],\n",
      "       [-0.05593975, -0.06565331, -0.02798287, -0.02651801, -0.14586152,\n",
      "        -0.00841928, -0.04840755, -0.03176909,  0.09586818,  0.05043178,\n",
      "        -0.07923994,  0.05774188],\n",
      "       [-0.06028375, -0.05106468,  0.03528098, -0.0228969 , -0.10756211,\n",
      "        -0.05044096, -0.01593054, -0.04240266,  0.00609364, -0.00251503,\n",
      "        -0.00659187, -0.17243148],\n",
      "       [ 0.24490115,  0.2497941 , -0.18767959,  0.23072621, -0.35442364,\n",
      "         0.22604693,  0.20054565,  0.26461235, -0.12465451, -0.20401609,\n",
      "         0.27384526, -0.24391817],\n",
      "       [ 0.02744445, -0.01923213, -0.1340106 ,  0.03663278, -0.3127395 ,\n",
      "         0.07666659, -0.00129619,  0.0190518 , -0.21004973, -0.13211122,\n",
      "         0.05187495, -0.16338122],\n",
      "       [-0.06192775, -0.05468928, -0.50433296, -0.04579021, -0.02559706,\n",
      "        -0.09056343,  0.01184863,  0.03283259, -0.41032314, -0.50903994,\n",
      "        -0.05177034, -0.30752707]], dtype=float32), array([0.0775158 , 0.14862965, 0.3105204 , 0.14996812, 0.39110532,\n",
      "       0.09626226, 0.03730651, 0.18361309, 0.29858482, 0.30431548,\n",
      "       0.0990803 , 0.3120722 ], dtype=float32)]\n",
      "[array([[ 4.88603055e-01, -1.39634579e-01, -1.20403267e-01,\n",
      "        -4.29798327e-02, -1.43813446e-01, -6.29433021e-02,\n",
      "        -2.07203645e-02, -1.41380250e-01],\n",
      "       [ 5.14542282e-01, -1.19464524e-01, -5.39323986e-02,\n",
      "        -2.70331986e-02, -5.32958135e-02, -6.57358393e-02,\n",
      "         3.59112322e-02, -1.37312084e-01],\n",
      "       [-2.40291767e-02,  4.27175194e-01, -1.24355808e-01,\n",
      "         3.91243678e-03,  4.27675515e-01, -4.88214307e-02,\n",
      "         1.73876360e-02,  3.92250806e-01],\n",
      "       [ 4.73289520e-01, -3.98916602e-02, -1.14923157e-01,\n",
      "        -4.68147025e-02, -8.17859843e-02, -2.32341960e-02,\n",
      "        -2.55329423e-02, -1.50789013e-02],\n",
      "       [-1.21750340e-01,  3.29483300e-01,  8.83779228e-02,\n",
      "        -3.14506143e-03,  3.86943191e-01,  5.96125908e-02,\n",
      "        -4.01323475e-02,  3.45749229e-01],\n",
      "       [ 5.13909101e-01, -1.53768614e-01, -8.78721103e-02,\n",
      "         1.12451492e-02, -1.48655817e-01, -1.80163849e-02,\n",
      "        -1.28495349e-02, -1.45392165e-01],\n",
      "       [ 4.24153596e-01, -1.20573074e-01, -9.39557254e-02,\n",
      "         7.54795223e-03, -1.09238736e-01, -3.04768160e-02,\n",
      "        -1.58709623e-02, -1.10329971e-01],\n",
      "       [ 4.86675143e-01,  2.02325135e-02,  6.23827390e-02,\n",
      "        -3.30312736e-02, -1.68857835e-02, -7.17817694e-02,\n",
      "        -1.31842103e-02,  7.83136562e-02],\n",
      "       [-7.00588077e-02,  3.91222388e-01, -3.02961096e-02,\n",
      "         1.42223854e-02,  3.75826001e-01, -2.00613576e-04,\n",
      "         1.48498211e-02,  3.95762146e-01],\n",
      "       [-1.06355622e-02,  3.87494296e-01, -1.03016771e-01,\n",
      "        -2.79413313e-02,  4.56215829e-01, -3.50485556e-02,\n",
      "        -1.30838435e-02,  3.84692609e-01],\n",
      "       [ 5.12414932e-01, -8.39176401e-02, -1.44204691e-01,\n",
      "         1.97256859e-02, -1.35034963e-01, -8.93346407e-03,\n",
      "        -3.14403549e-02, -9.27149430e-02],\n",
      "       [-8.84819031e-03,  3.41740489e-01, -1.96056783e-01,\n",
      "        -2.19767587e-03,  3.60928595e-01,  7.15840757e-02,\n",
      "        -3.67093682e-02,  2.60698020e-01]], dtype=float32), array([ 0.16937453,  0.32755536,  0.09529392, -0.01192565,  0.3313844 ,\n",
      "       -0.04564217, -0.01246555,  0.26554975], dtype=float32)]\n",
      "[array([[ 0.49731687],\n",
      "       [-0.41962618],\n",
      "       [-0.3335389 ],\n",
      "       [ 0.02054305],\n",
      "       [-0.4107658 ],\n",
      "       [ 0.12043741],\n",
      "       [ 0.03139787],\n",
      "       [-0.36441836]], dtype=float32), array([-0.09126358], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "for layer in model.layers:\n",
    "    weights = layer.get_weights()\n",
    "    print(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)  \n",
    "p_labels = [round(x[0]) for x in predictions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 1.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7077922077922078\n",
      "[[81 19]\n",
      " [26 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.81      0.78       100\n",
      "         1.0       0.60      0.52      0.55        54\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.68      0.66      0.67       154\n",
      "weighted avg       0.70      0.71      0.70       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(y_test, p_labels))\n",
    "print(confusion_matrix(y_test,p_labels))  \n",
    "print(classification_report(y_test,p_labels))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 16)                144       \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 8)                 136       \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 289\n",
      "Trainable params: 289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dropout\n",
    "\n",
    "# create keras model\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(16, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(8, kernel_initializer='normal', activation='relu'))\n",
    "model2.add(Dense(1, kernel_initializer='normal', activation='sigmoid'))\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 460 samples, validate on 154 samples\n",
      "Epoch 1/60\n",
      "460/460 [==============================] - 0s 683us/step - loss: 0.6836 - accuracy: 0.6500 - val_loss: 0.6773 - val_accuracy: 0.6364\n",
      "Epoch 2/60\n",
      "460/460 [==============================] - 0s 451us/step - loss: 0.6690 - accuracy: 0.6565 - val_loss: 0.6680 - val_accuracy: 0.6364\n",
      "Epoch 3/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6600 - accuracy: 0.6565 - val_loss: 0.6625 - val_accuracy: 0.6364\n",
      "Epoch 4/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6540 - accuracy: 0.6565 - val_loss: 0.6590 - val_accuracy: 0.6364\n",
      "Epoch 5/60\n",
      "460/460 [==============================] - 0s 449us/step - loss: 0.6501 - accuracy: 0.6565 - val_loss: 0.6570 - val_accuracy: 0.6364\n",
      "Epoch 6/60\n",
      "460/460 [==============================] - 0s 447us/step - loss: 0.6476 - accuracy: 0.6565 - val_loss: 0.6558 - val_accuracy: 0.6364\n",
      "Epoch 7/60\n",
      "460/460 [==============================] - 0s 449us/step - loss: 0.6460 - accuracy: 0.6565 - val_loss: 0.6552 - val_accuracy: 0.6364\n",
      "Epoch 8/60\n",
      "460/460 [==============================] - 0s 449us/step - loss: 0.6449 - accuracy: 0.6565 - val_loss: 0.6548 - val_accuracy: 0.6364\n",
      "Epoch 9/60\n",
      "460/460 [==============================] - 0s 453us/step - loss: 0.6442 - accuracy: 0.6565 - val_loss: 0.6547 - val_accuracy: 0.6364\n",
      "Epoch 10/60\n",
      "460/460 [==============================] - 0s 473us/step - loss: 0.6437 - accuracy: 0.6565 - val_loss: 0.6546 - val_accuracy: 0.6364\n",
      "Epoch 11/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.6432 - accuracy: 0.6565 - val_loss: 0.6545 - val_accuracy: 0.6364\n",
      "Epoch 12/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.6430 - accuracy: 0.6565 - val_loss: 0.6545 - val_accuracy: 0.6364\n",
      "Epoch 13/60\n",
      "460/460 [==============================] - 0s 451us/step - loss: 0.6427 - accuracy: 0.6565 - val_loss: 0.6545 - val_accuracy: 0.6364\n",
      "Epoch 14/60\n",
      "460/460 [==============================] - 0s 451us/step - loss: 0.6426 - accuracy: 0.6565 - val_loss: 0.6544 - val_accuracy: 0.6364\n",
      "Epoch 15/60\n",
      "460/460 [==============================] - 0s 460us/step - loss: 0.6424 - accuracy: 0.6565 - val_loss: 0.6543 - val_accuracy: 0.6364\n",
      "Epoch 16/60\n",
      "460/460 [==============================] - 0s 447us/step - loss: 0.6422 - accuracy: 0.6565 - val_loss: 0.6542 - val_accuracy: 0.6364\n",
      "Epoch 17/60\n",
      "460/460 [==============================] - 0s 440us/step - loss: 0.6420 - accuracy: 0.6565 - val_loss: 0.6541 - val_accuracy: 0.6364\n",
      "Epoch 18/60\n",
      "460/460 [==============================] - 0s 475us/step - loss: 0.6418 - accuracy: 0.6565 - val_loss: 0.6539 - val_accuracy: 0.6364\n",
      "Epoch 19/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6416 - accuracy: 0.6565 - val_loss: 0.6536 - val_accuracy: 0.6364\n",
      "Epoch 20/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6413 - accuracy: 0.6565 - val_loss: 0.6533 - val_accuracy: 0.6364\n",
      "Epoch 21/60\n",
      "460/460 [==============================] - 0s 449us/step - loss: 0.6410 - accuracy: 0.6565 - val_loss: 0.6529 - val_accuracy: 0.6364\n",
      "Epoch 22/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6407 - accuracy: 0.6565 - val_loss: 0.6525 - val_accuracy: 0.6364\n",
      "Epoch 23/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.6403 - accuracy: 0.6565 - val_loss: 0.6521 - val_accuracy: 0.6364\n",
      "Epoch 24/60\n",
      "460/460 [==============================] - 0s 466us/step - loss: 0.6398 - accuracy: 0.6565 - val_loss: 0.6515 - val_accuracy: 0.6364\n",
      "Epoch 25/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.6394 - accuracy: 0.6565 - val_loss: 0.6508 - val_accuracy: 0.6364\n",
      "Epoch 26/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.6387 - accuracy: 0.6565 - val_loss: 0.6500 - val_accuracy: 0.6364\n",
      "Epoch 27/60\n",
      "460/460 [==============================] - 0s 475us/step - loss: 0.6380 - accuracy: 0.6565 - val_loss: 0.6491 - val_accuracy: 0.6364\n",
      "Epoch 28/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6371 - accuracy: 0.6565 - val_loss: 0.6480 - val_accuracy: 0.6364\n",
      "Epoch 29/60\n",
      "460/460 [==============================] - 0s 460us/step - loss: 0.6360 - accuracy: 0.6565 - val_loss: 0.6466 - val_accuracy: 0.6364\n",
      "Epoch 30/60\n",
      "460/460 [==============================] - 0s 455us/step - loss: 0.6349 - accuracy: 0.6565 - val_loss: 0.6450 - val_accuracy: 0.6364\n",
      "Epoch 31/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6333 - accuracy: 0.6565 - val_loss: 0.6430 - val_accuracy: 0.6364\n",
      "Epoch 32/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.6314 - accuracy: 0.6565 - val_loss: 0.6406 - val_accuracy: 0.6364\n",
      "Epoch 33/60\n",
      "460/460 [==============================] - 0s 453us/step - loss: 0.6292 - accuracy: 0.6565 - val_loss: 0.6376 - val_accuracy: 0.6364\n",
      "Epoch 34/60\n",
      "460/460 [==============================] - 0s 453us/step - loss: 0.6265 - accuracy: 0.6565 - val_loss: 0.6339 - val_accuracy: 0.6364\n",
      "Epoch 35/60\n",
      "460/460 [==============================] - 0s 464us/step - loss: 0.6229 - accuracy: 0.6565 - val_loss: 0.6294 - val_accuracy: 0.6364\n",
      "Epoch 36/60\n",
      "460/460 [==============================] - 0s 460us/step - loss: 0.6187 - accuracy: 0.6565 - val_loss: 0.6236 - val_accuracy: 0.6364\n",
      "Epoch 37/60\n",
      "460/460 [==============================] - 0s 466us/step - loss: 0.6132 - accuracy: 0.6565 - val_loss: 0.6166 - val_accuracy: 0.6364\n",
      "Epoch 38/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.6066 - accuracy: 0.6565 - val_loss: 0.6077 - val_accuracy: 0.6364\n",
      "Epoch 39/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.5982 - accuracy: 0.6565 - val_loss: 0.5969 - val_accuracy: 0.6364\n",
      "Epoch 40/60\n",
      "460/460 [==============================] - 0s 464us/step - loss: 0.5881 - accuracy: 0.6543 - val_loss: 0.5839 - val_accuracy: 0.6364\n",
      "Epoch 41/60\n",
      "460/460 [==============================] - 0s 464us/step - loss: 0.5760 - accuracy: 0.6674 - val_loss: 0.5684 - val_accuracy: 0.6753\n",
      "Epoch 42/60\n",
      "460/460 [==============================] - 0s 449us/step - loss: 0.5623 - accuracy: 0.6978 - val_loss: 0.5517 - val_accuracy: 0.7338\n",
      "Epoch 43/60\n",
      "460/460 [==============================] - 0s 451us/step - loss: 0.5473 - accuracy: 0.7370 - val_loss: 0.5336 - val_accuracy: 0.7597\n",
      "Epoch 44/60\n",
      "460/460 [==============================] - 0s 457us/step - loss: 0.5324 - accuracy: 0.7717 - val_loss: 0.5164 - val_accuracy: 0.7727\n",
      "Epoch 45/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.5188 - accuracy: 0.7826 - val_loss: 0.5010 - val_accuracy: 0.7727\n",
      "Epoch 46/60\n",
      "460/460 [==============================] - 0s 460us/step - loss: 0.5063 - accuracy: 0.7826 - val_loss: 0.4899 - val_accuracy: 0.7792\n",
      "Epoch 47/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.4964 - accuracy: 0.7848 - val_loss: 0.4803 - val_accuracy: 0.7792\n",
      "Epoch 48/60\n",
      "460/460 [==============================] - 0s 466us/step - loss: 0.4883 - accuracy: 0.7848 - val_loss: 0.4722 - val_accuracy: 0.7857\n",
      "Epoch 49/60\n",
      "460/460 [==============================] - 0s 449us/step - loss: 0.4813 - accuracy: 0.7870 - val_loss: 0.4649 - val_accuracy: 0.7922\n",
      "Epoch 50/60\n",
      "460/460 [==============================] - 0s 464us/step - loss: 0.4766 - accuracy: 0.7870 - val_loss: 0.4616 - val_accuracy: 0.7922\n",
      "Epoch 51/60\n",
      "460/460 [==============================] - 0s 460us/step - loss: 0.4725 - accuracy: 0.7978 - val_loss: 0.4592 - val_accuracy: 0.7922\n",
      "Epoch 52/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.4694 - accuracy: 0.7913 - val_loss: 0.4561 - val_accuracy: 0.7922\n",
      "Epoch 53/60\n",
      "460/460 [==============================] - 0s 468us/step - loss: 0.4671 - accuracy: 0.7935 - val_loss: 0.4543 - val_accuracy: 0.7922\n",
      "Epoch 54/60\n",
      "460/460 [==============================] - 0s 462us/step - loss: 0.4644 - accuracy: 0.7913 - val_loss: 0.4534 - val_accuracy: 0.7922\n",
      "Epoch 55/60\n",
      "460/460 [==============================] - 0s 473us/step - loss: 0.4623 - accuracy: 0.7891 - val_loss: 0.4505 - val_accuracy: 0.7792\n",
      "Epoch 56/60\n",
      "460/460 [==============================] - 0s 473us/step - loss: 0.4615 - accuracy: 0.7870 - val_loss: 0.4499 - val_accuracy: 0.7922\n",
      "Epoch 57/60\n",
      "460/460 [==============================] - 0s 464us/step - loss: 0.4602 - accuracy: 0.7957 - val_loss: 0.4490 - val_accuracy: 0.7922\n",
      "Epoch 58/60\n",
      "460/460 [==============================] - 0s 460us/step - loss: 0.4591 - accuracy: 0.7913 - val_loss: 0.4471 - val_accuracy: 0.7857\n",
      "Epoch 59/60\n",
      "460/460 [==============================] - 0s 468us/step - loss: 0.4580 - accuracy: 0.7848 - val_loss: 0.4464 - val_accuracy: 0.7922\n",
      "Epoch 60/60\n",
      "460/460 [==============================] - 0s 447us/step - loss: 0.4571 - accuracy: 0.7891 - val_loss: 0.4458 - val_accuracy: 0.7922\n"
     ]
    }
   ],
   "source": [
    "# Compile model\n",
    "model2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "# Fit the model\n",
    "history=model2.fit(X_train, y_train, validation_split=0.25, epochs=60, batch_size=5)\n",
    "# calculate predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model2.predict(X_test)  \n",
    "pred_labels = [round(x[0]) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7077922077922078\n",
      "[[81 19]\n",
      " [26 28]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.76      0.81      0.78       100\n",
      "         1.0       0.60      0.52      0.55        54\n",
      "\n",
      "    accuracy                           0.71       154\n",
      "   macro avg       0.68      0.66      0.67       154\n",
      "weighted avg       0.70      0.71      0.70       154\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\", metrics.accuracy_score(y_test, pred_labels))\n",
    "print(confusion_matrix(y_test,pred_labels))  \n",
    "print(classification_report(y_test,pred_labels))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
